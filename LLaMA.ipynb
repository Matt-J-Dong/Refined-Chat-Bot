{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!module list\n",
    "#!module avail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'llama' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/facebookresearch/llama.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: fire in /home/mjd9571/.local/lib/python3.8/site-packages (0.5.0)\n",
      "Requirement already satisfied: six in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from fire) (1.15.0)\n",
      "Requirement already satisfied: termcolor in /home/mjd9571/.local/lib/python3.8/site-packages (from fire) (2.3.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/share/apps/python/3.8.6/intel/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: fairscale in /home/mjd9571/.local/lib/python3.8/site-packages (0.4.13)\n",
      "Requirement already satisfied: torch>=1.8.0 in /home/mjd9571/.local/lib/python3.8/site-packages (from fairscale) (2.1.1)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /home/mjd9571/.local/lib/python3.8/site-packages (from fairscale) (1.24.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/mjd9571/.local/lib/python3.8/site-packages (from torch>=1.8.0->fairscale) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/mjd9571/.local/lib/python3.8/site-packages (from torch>=1.8.0->fairscale) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/mjd9571/.local/lib/python3.8/site-packages (from torch>=1.8.0->fairscale) (2.1.0)\n",
      "Requirement already satisfied: filelock in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from torch>=1.8.0->fairscale) (3.0.12)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/mjd9571/.local/lib/python3.8/site-packages (from torch>=1.8.0->fairscale) (11.4.5.107)\n",
      "Requirement already satisfied: networkx in /home/mjd9571/.local/lib/python3.8/site-packages (from torch>=1.8.0->fairscale) (3.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/mjd9571/.local/lib/python3.8/site-packages (from torch>=1.8.0->fairscale) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/mjd9571/.local/lib/python3.8/site-packages (from torch>=1.8.0->fairscale) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/mjd9571/.local/lib/python3.8/site-packages (from torch>=1.8.0->fairscale) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/mjd9571/.local/lib/python3.8/site-packages (from torch>=1.8.0->fairscale) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/mjd9571/.local/lib/python3.8/site-packages (from torch>=1.8.0->fairscale) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/mjd9571/.local/lib/python3.8/site-packages (from torch>=1.8.0->fairscale) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/mjd9571/.local/lib/python3.8/site-packages (from torch>=1.8.0->fairscale) (12.1.105)\n",
      "Requirement already satisfied: sympy in /home/mjd9571/.local/lib/python3.8/site-packages (from torch>=1.8.0->fairscale) (1.12)\n",
      "Requirement already satisfied: fsspec in /home/mjd9571/.local/lib/python3.8/site-packages (from torch>=1.8.0->fairscale) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/mjd9571/.local/lib/python3.8/site-packages (from torch>=1.8.0->fairscale) (2.18.1)\n",
      "Requirement already satisfied: jinja2 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from torch>=1.8.0->fairscale) (2.11.2)\n",
      "Requirement already satisfied: typing-extensions in /home/mjd9571/.local/lib/python3.8/site-packages (from torch>=1.8.0->fairscale) (4.8.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/mjd9571/.local/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch>=1.8.0->fairscale) (12.3.101)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/mjd9571/.local/lib/python3.8/site-packages (from sympy->torch>=1.8.0->fairscale) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from jinja2->torch>=1.8.0->fairscale) (1.1.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/share/apps/python/3.8.6/intel/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentencepiece in /home/mjd9571/.local/lib/python3.8/site-packages (0.1.99)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/share/apps/python/3.8.6/intel/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install fire\n",
    "!pip install fairscale\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/mjd9571/llama\n",
      "CODE_OF_CONDUCT.md  Responsible-Use-Guide.pdf\texample_text_completion.py\n",
      "CONTRIBUTING.md     UPDATES.md\t\t\tllama\n",
      "LICENSE\t\t    USE_POLICY.md\t\trequirements.txt\n",
      "MODEL_CARD.md\t    download.sh\t\t\tsetup.py\n",
      "README.md\t    example_chat_completion.py\n",
      "\n",
      "Downloading LICENSE and Acceptable Usage Policy\n",
      "--2023-11-30 18:11:23--  https://download.llamameta.net/LICENSE?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiNTd2aHBnNXVhaXZwbTluNHp1Ymd4Y3V1IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMTQxNDM1MH19fV19&Signature=WOIasx4uqHXVGI6qPGD7kygxdYG5StmgqOjmewhLV2YqDKymNXKeS33IuW%7Em82FJcLkgp5fa%7EtgCnbFr2AnlPugaFrKWPqfZpy5peukoGkaPElEcMn5ci-37tXDBeAa9iEDCrRCLBPVhqDuDAL0LETs7V-k4E6pPwW8vu0PUrXFrU7Q2nffpJZkhv2Hnb7cLxmdNxvkKObKpzRjmlhPNsHLE0iSVvfwGlFXeZ-P07wtfCNxxpvEs6Tge5BqYzRJP1T6EAquKwfmg90Z8I29gTaQBl36wjA98RLUMSPS-gLegkID8Zg9xr508qfLRe4AwhqPaLNt4PZvDA4SAj6GX4g__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=325316320384449\n",
      "Resolving download.llamameta.net (download.llamameta.net)... 108.138.106.52, 108.138.106.87, 108.138.106.23, ...\n",
      "Connecting to download.llamameta.net (download.llamameta.net)|108.138.106.52|:443... connected.\n",
      "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
      "\n",
      "    The file is already fully retrieved; nothing to do.\n",
      "\n",
      "--2023-11-30 18:11:24--  https://download.llamameta.net/USE_POLICY.md?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiNTd2aHBnNXVhaXZwbTluNHp1Ymd4Y3V1IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMTQxNDM1MH19fV19&Signature=WOIasx4uqHXVGI6qPGD7kygxdYG5StmgqOjmewhLV2YqDKymNXKeS33IuW%7Em82FJcLkgp5fa%7EtgCnbFr2AnlPugaFrKWPqfZpy5peukoGkaPElEcMn5ci-37tXDBeAa9iEDCrRCLBPVhqDuDAL0LETs7V-k4E6pPwW8vu0PUrXFrU7Q2nffpJZkhv2Hnb7cLxmdNxvkKObKpzRjmlhPNsHLE0iSVvfwGlFXeZ-P07wtfCNxxpvEs6Tge5BqYzRJP1T6EAquKwfmg90Z8I29gTaQBl36wjA98RLUMSPS-gLegkID8Zg9xr508qfLRe4AwhqPaLNt4PZvDA4SAj6GX4g__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=325316320384449\n",
      "Resolving download.llamameta.net (download.llamameta.net)... 108.138.106.23, 108.138.106.50, 108.138.106.87, ...\n",
      "Connecting to download.llamameta.net (download.llamameta.net)|108.138.106.23|:443... connected.\n",
      "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
      "\n",
      "    The file is already fully retrieved; nothing to do.\n",
      "\n",
      "Downloading tokenizer\n",
      "--2023-11-30 18:11:24--  https://download.llamameta.net/tokenizer.model?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiNTd2aHBnNXVhaXZwbTluNHp1Ymd4Y3V1IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMTQxNDM1MH19fV19&Signature=WOIasx4uqHXVGI6qPGD7kygxdYG5StmgqOjmewhLV2YqDKymNXKeS33IuW%7Em82FJcLkgp5fa%7EtgCnbFr2AnlPugaFrKWPqfZpy5peukoGkaPElEcMn5ci-37tXDBeAa9iEDCrRCLBPVhqDuDAL0LETs7V-k4E6pPwW8vu0PUrXFrU7Q2nffpJZkhv2Hnb7cLxmdNxvkKObKpzRjmlhPNsHLE0iSVvfwGlFXeZ-P07wtfCNxxpvEs6Tge5BqYzRJP1T6EAquKwfmg90Z8I29gTaQBl36wjA98RLUMSPS-gLegkID8Zg9xr508qfLRe4AwhqPaLNt4PZvDA4SAj6GX4g__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=325316320384449\n",
      "Resolving download.llamameta.net (download.llamameta.net)... 108.138.106.52, 108.138.106.50, 108.138.106.87, ...\n",
      "Connecting to download.llamameta.net (download.llamameta.net)|108.138.106.52|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 499723 (488K) [binary/octet-stream]\n",
      "Saving to: ‘./tokenizer.model’\n",
      "\n",
      "./tokenizer.model   100%[===================>] 488.01K  --.-KB/s    in 0.008s  \n",
      "\n",
      "2023-11-30 18:11:25 (62.5 MB/s) - ‘./tokenizer.model’ saved [499723/499723]\n",
      "\n",
      "--2023-11-30 18:11:25--  https://download.llamameta.net/tokenizer_checklist.chk?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiNTd2aHBnNXVhaXZwbTluNHp1Ymd4Y3V1IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMTQxNDM1MH19fV19&Signature=WOIasx4uqHXVGI6qPGD7kygxdYG5StmgqOjmewhLV2YqDKymNXKeS33IuW%7Em82FJcLkgp5fa%7EtgCnbFr2AnlPugaFrKWPqfZpy5peukoGkaPElEcMn5ci-37tXDBeAa9iEDCrRCLBPVhqDuDAL0LETs7V-k4E6pPwW8vu0PUrXFrU7Q2nffpJZkhv2Hnb7cLxmdNxvkKObKpzRjmlhPNsHLE0iSVvfwGlFXeZ-P07wtfCNxxpvEs6Tge5BqYzRJP1T6EAquKwfmg90Z8I29gTaQBl36wjA98RLUMSPS-gLegkID8Zg9xr508qfLRe4AwhqPaLNt4PZvDA4SAj6GX4g__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=325316320384449\n",
      "Resolving download.llamameta.net (download.llamameta.net)... 108.138.106.50, 108.138.106.52, 108.138.106.87, ...\n",
      "Connecting to download.llamameta.net (download.llamameta.net)|108.138.106.50|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 50 [binary/octet-stream]\n",
      "Saving to: ‘./tokenizer_checklist.chk’\n",
      "\n",
      "./tokenizer_checkli 100%[===================>]      50  --.-KB/s    in 0s      \n",
      "\n",
      "2023-11-30 18:11:25 (127 KB/s) - ‘./tokenizer_checklist.chk’ saved [50/50]\n",
      "\n",
      "tokenizer.model: OK\n",
      "Downloading llama-2-7b-chat\n",
      "--2023-11-30 18:11:25--  https://download.llamameta.net/llama-2-7b-chat/consolidated.00.pth?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiNTd2aHBnNXVhaXZwbTluNHp1Ymd4Y3V1IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMTQxNDM1MH19fV19&Signature=WOIasx4uqHXVGI6qPGD7kygxdYG5StmgqOjmewhLV2YqDKymNXKeS33IuW%7Em82FJcLkgp5fa%7EtgCnbFr2AnlPugaFrKWPqfZpy5peukoGkaPElEcMn5ci-37tXDBeAa9iEDCrRCLBPVhqDuDAL0LETs7V-k4E6pPwW8vu0PUrXFrU7Q2nffpJZkhv2Hnb7cLxmdNxvkKObKpzRjmlhPNsHLE0iSVvfwGlFXeZ-P07wtfCNxxpvEs6Tge5BqYzRJP1T6EAquKwfmg90Z8I29gTaQBl36wjA98RLUMSPS-gLegkID8Zg9xr508qfLRe4AwhqPaLNt4PZvDA4SAj6GX4g__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=325316320384449\n",
      "Resolving download.llamameta.net (download.llamameta.net)... 108.138.106.50, 108.138.106.52, 108.138.106.87, ...\n",
      "Connecting to download.llamameta.net (download.llamameta.net)|108.138.106.50|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13476925163 (13G) [binary/octet-stream]\n",
      "Saving to: ‘./llama-2-7b-chat/consolidated.00.pth’\n",
      "\n",
      "./llama-2-7b-chat/c 100%[===================>]  12.55G   200MB/s    in 71s     \n",
      "\n",
      "2023-11-30 18:12:37 (181 MB/s) - ‘./llama-2-7b-chat/consolidated.00.pth’ saved [13476925163/13476925163]\n",
      "\n",
      "--2023-11-30 18:12:37--  https://download.llamameta.net/llama-2-7b-chat/params.json?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiNTd2aHBnNXVhaXZwbTluNHp1Ymd4Y3V1IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMTQxNDM1MH19fV19&Signature=WOIasx4uqHXVGI6qPGD7kygxdYG5StmgqOjmewhLV2YqDKymNXKeS33IuW%7Em82FJcLkgp5fa%7EtgCnbFr2AnlPugaFrKWPqfZpy5peukoGkaPElEcMn5ci-37tXDBeAa9iEDCrRCLBPVhqDuDAL0LETs7V-k4E6pPwW8vu0PUrXFrU7Q2nffpJZkhv2Hnb7cLxmdNxvkKObKpzRjmlhPNsHLE0iSVvfwGlFXeZ-P07wtfCNxxpvEs6Tge5BqYzRJP1T6EAquKwfmg90Z8I29gTaQBl36wjA98RLUMSPS-gLegkID8Zg9xr508qfLRe4AwhqPaLNt4PZvDA4SAj6GX4g__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=325316320384449\n",
      "Resolving download.llamameta.net (download.llamameta.net)... 108.138.106.87, 108.138.106.23, 108.138.106.52, ...\n",
      "Connecting to download.llamameta.net (download.llamameta.net)|108.138.106.87|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 102 [application/json]\n",
      "Saving to: ‘./llama-2-7b-chat/params.json’\n",
      "\n",
      "./llama-2-7b-chat/p 100%[===================>]     102  --.-KB/s    in 0s      \n",
      "\n",
      "2023-11-30 18:12:37 (10.2 MB/s) - ‘./llama-2-7b-chat/params.json’ saved [102/102]\n",
      "\n",
      "--2023-11-30 18:12:37--  https://download.llamameta.net/llama-2-7b-chat/checklist.chk?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiNTd2aHBnNXVhaXZwbTluNHp1Ymd4Y3V1IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMTQxNDM1MH19fV19&Signature=WOIasx4uqHXVGI6qPGD7kygxdYG5StmgqOjmewhLV2YqDKymNXKeS33IuW%7Em82FJcLkgp5fa%7EtgCnbFr2AnlPugaFrKWPqfZpy5peukoGkaPElEcMn5ci-37tXDBeAa9iEDCrRCLBPVhqDuDAL0LETs7V-k4E6pPwW8vu0PUrXFrU7Q2nffpJZkhv2Hnb7cLxmdNxvkKObKpzRjmlhPNsHLE0iSVvfwGlFXeZ-P07wtfCNxxpvEs6Tge5BqYzRJP1T6EAquKwfmg90Z8I29gTaQBl36wjA98RLUMSPS-gLegkID8Zg9xr508qfLRe4AwhqPaLNt4PZvDA4SAj6GX4g__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=325316320384449\n",
      "Resolving download.llamameta.net (download.llamameta.net)... 108.138.106.87, 108.138.106.50, 108.138.106.52, ...\n",
      "Connecting to download.llamameta.net (download.llamameta.net)|108.138.106.87|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 100 [binary/octet-stream]\n",
      "Saving to: ‘./llama-2-7b-chat/checklist.chk’\n",
      "\n",
      "./llama-2-7b-chat/c 100%[===================>]     100  --.-KB/s    in 0s      \n",
      "\n",
      "2023-11-30 18:12:38 (797 KB/s) - ‘./llama-2-7b-chat/checklist.chk’ saved [100/100]\n",
      "\n",
      "Checking checksums\n",
      "consolidated.00.pth: OK\n",
      "params.json: OK\n"
     ]
    }
   ],
   "source": [
    "%cd llama\n",
    "!ls\n",
    "!./download.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOYLECAKE\n"
     ]
    }
   ],
   "source": [
    "print(\"YOYLECAKE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/mjd9571/llama\n"
     ]
    }
   ],
   "source": [
    "#%cd llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checklist.chk  consolidated.00.pth  params.json\n",
      "CODE_OF_CONDUCT.md\t   UPDATES.md\t\t       llama-2-7b-chat\n",
      "CONTRIBUTING.md\t\t   USE_POLICY.md\t       requirements.txt\n",
      "LICENSE\t\t\t   download.sh\t\t       setup.py\n",
      "MODEL_CARD.md\t\t   example_chat_completion.py  tokenizer.model\n",
      "README.md\t\t   example_text_completion.py  tokenizer_checklist.chk\n",
      "Responsible-Use-Guide.pdf  llama\n"
     ]
    }
   ],
   "source": [
    "#%cd llama-2-7b/\n",
    "!ls llama-2-7b-chat\n",
    "#%cd ..\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.1.1\n",
      "Uninstalling torch-2.1.1:\n",
      "  Successfully uninstalled torch-2.1.1\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torch\n",
      "  Using cached torch-2.1.1-cp38-cp38-manylinux1_x86_64.whl (670.2 MB)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/mjd9571/.local/lib/python3.8/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/mjd9571/.local/lib/python3.8/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: jinja2 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from torch) (2.11.2)\n",
      "Requirement already satisfied: fsspec in /home/mjd9571/.local/lib/python3.8/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: sympy in /home/mjd9571/.local/lib/python3.8/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: triton==2.1.0; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/mjd9571/.local/lib/python3.8/site-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/mjd9571/.local/lib/python3.8/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/mjd9571/.local/lib/python3.8/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/mjd9571/.local/lib/python3.8/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/mjd9571/.local/lib/python3.8/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/mjd9571/.local/lib/python3.8/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/mjd9571/.local/lib/python3.8/site-packages (from torch) (2.18.1)\n",
      "Requirement already satisfied: networkx in /home/mjd9571/.local/lib/python3.8/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/mjd9571/.local/lib/python3.8/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: typing-extensions in /home/mjd9571/.local/lib/python3.8/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/mjd9571/.local/lib/python3.8/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: filelock in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from torch) (3.0.12)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/mjd9571/.local/lib/python3.8/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from jinja2->torch) (1.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/mjd9571/.local/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/mjd9571/.local/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch) (12.3.101)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-2.1.1\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/share/apps/python/3.8.6/intel/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip uninstall torch -y\n",
    "#!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\u001b[33mWARNING: Could not generate requirement for distribution -ip 20.2.1 (/share/apps/python/3.8.6/intel/lib/python3.8/site-packages): Parse error at \"'-ip==20.'\": Expected W:(abcd...)\u001b[0m\n",
      "absl-py==0.13.0\n",
      "accelerate==0.24.1\n",
      "alembic==1.4.3\n",
      "anyio==2.0.2\n",
      "appdirs==1.4.4\n",
      "argon2-cffi==20.1.0\n",
      "async-generator==1.10\n",
      "attrs==20.2.0\n",
      "Babel==2.9.0\n",
      "backcall==0.2.0\n",
      "bash-kernel==0.7.2\n",
      "bleach==3.2.1\n",
      "certifi==2020.6.20\n",
      "certipy==0.1.3\n",
      "cffi==1.14.3\n",
      "chardet==3.0.4\n",
      "cryptography==3.1.1\n",
      "cycler==0.10.0\n",
      "Cython==0.29.21\n",
      "decorator==4.4.2\n",
      "defusedxml==0.6.0\n",
      "distlib==0.3.1\n",
      "entrypoints==0.3\n",
      "fairscale==0.4.13\n",
      "filelock==3.0.12\n",
      "fire==0.5.0\n",
      "fsspec==2023.10.0\n",
      "huggingface-hub==0.19.4\n",
      "idna==2.10\n",
      "ipykernel==5.3.4\n",
      "ipython==7.18.1\n",
      "ipython-genutils==0.2.0\n",
      "ipywidgets==7.5.1\n",
      "jedi==0.17.2\n",
      "Jinja2==2.11.2\n",
      "json5==0.9.5\n",
      "jsonschema==3.2.0\n",
      "jupyter==1.0.0\n",
      "jupyter-client==6.1.7\n",
      "jupyter-console==6.2.0\n",
      "jupyter-contrib-core==0.3.3\n",
      "jupyter-contrib-nbextensions==0.5.1\n",
      "jupyter-core==4.6.3\n",
      "jupyter-highlight-selected-word==0.2.0\n",
      "jupyter-latex-envs==1.4.6\n",
      "jupyter-nbextensions-configurator==0.4.1\n",
      "jupyter-server==1.2.2\n",
      "jupyter-telemetry==0.1.0\n",
      "jupyterhub==1.1.0\n",
      "jupyterlab==3.0.5\n",
      "jupyterlab-mathjax3==4.3.0\n",
      "jupyterlab-pygments==0.1.2\n",
      "jupyterlab-server==2.1.2\n",
      "kiwisolver==1.2.0\n",
      "lxml==4.6.2\n",
      "Mako==1.1.3\n",
      "MarkupSafe==1.1.1\n",
      "matplotlib==3.3.2\n",
      "meson==0.57.2\n",
      "mistune==0.8.4\n",
      "mpmath==1.3.0\n",
      "nbclassic==0.2.6\n",
      "nbclient==0.5.0\n",
      "nbconvert==6.0.7\n",
      "nbformat==5.0.7\n",
      "nest-asyncio==1.4.1\n",
      "networkx==3.1\n",
      "ninja==1.10.0.post2\n",
      "notebook==6.1.4\n",
      "numpy==1.24.4\n",
      "nvidia-cublas-cu12==12.1.3.1\n",
      "nvidia-cuda-cupti-cu12==12.1.105\n",
      "nvidia-cuda-nvrtc-cu12==12.1.105\n",
      "nvidia-cuda-runtime-cu12==12.1.105\n",
      "nvidia-cudnn-cu12==8.9.2.26\n",
      "nvidia-cufft-cu12==11.0.2.54\n",
      "nvidia-curand-cu12==10.3.2.106\n",
      "nvidia-cusolver-cu12==11.4.5.107\n",
      "nvidia-cusparse-cu12==12.1.0.106\n",
      "nvidia-nccl-cu12==2.18.1\n",
      "nvidia-nvjitlink-cu12==12.3.101\n",
      "nvidia-nvtx-cu12==12.1.105\n",
      "oauthlib==3.1.0\n",
      "packaging==20.4\n",
      "pamela==1.0.0\n",
      "pandas==1.1.3\n",
      "pandocfilters==1.4.2\n",
      "parso==0.7.1\n",
      "pexpect==4.8.0\n",
      "pickleshare==0.7.5\n",
      "Pillow==8.0.1\n",
      "prometheus-client==0.8.0\n",
      "prompt-toolkit==3.0.7\n",
      "psutil==5.8.0\n",
      "ptyprocess==0.6.0\n",
      "pycparser==2.20\n",
      "Pygments==2.7.1\n",
      "pyOpenSSL==19.1.0\n",
      "pyparsing==2.4.7\n",
      "pyrsistent==0.17.3\n",
      "python-dateutil==2.8.1\n",
      "python-editor==1.0.4\n",
      "python-json-logger==2.0.0\n",
      "pytz==2020.1\n",
      "PyYAML==5.3.1\n",
      "pyzmq==19.0.2\n",
      "qtconsole==4.7.7\n",
      "QtPy==1.9.0\n",
      "regex==2023.10.3\n",
      "requests==2.24.0\n",
      "ruamel.yaml==0.16.12\n",
      "ruamel.yaml.clib==0.2.2\n",
      "safetensors==0.4.1\n",
      "scipy==1.5.2\n",
      "SCons==4.0.1\n",
      "Send2Trash==1.5.0\n",
      "sentencepiece==0.1.99\n",
      "six==1.15.0\n",
      "sniffio==1.2.0\n",
      "SQLAlchemy==1.3.19\n",
      "sympy==1.12\n",
      "termcolor==2.3.0\n",
      "terminado==0.9.1\n",
      "testpath==0.4.4\n",
      "tokenizers==0.15.0\n",
      "torch==2.1.1\n",
      "tornado==6.1\n",
      "tqdm==4.66.1\n",
      "traitlets==5.0.4\n",
      "transformers==4.35.2\n",
      "triton==2.1.0\n",
      "typing-extensions==4.8.0\n",
      "urllib3==1.25.10\n",
      "virtualenv==20.0.33\n",
      "wcwidth==0.2.5\n",
      "webencodings==0.5.1\n",
      "websockify==0.9.0\n",
      "wget==3.2\n",
      "widgetsnbextension==3.5.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__file__)\n",
    "!python -m pip freeze\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'torch' from '/home/mjd9571/.local/lib/python3.8/site-packages/torch/__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch)\n",
    "tensor = torch.tensor([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA (GPU) is available.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA (GPU) is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"CUDA (GPU) is available.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA (GPU) is not available, falling back to CPU.\")\n",
    "\n",
    "# You can now use 'device' to move tensors and models to the selected device.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.__version__ # Get PyTorch and CUDA version\n",
    "torch.cuda.is_available() # Check that CUDA works\n",
    "torch.cuda.device_count() # Check how many CUDA capable devices you have\n",
    "\n",
    "# Print device human readable names\n",
    "torch.cuda.get_device_name(0)\n",
    "#torch.cuda.get_device_name(1)\n",
    "# Add more lines with +1 like get_device_name(3), get_device_name(4) if you have more devices.\n",
    "#CUDA_DEVICE_ORDER=PCI_BUS_ID !python neural_style.py\n",
    "# Only make GPU ID 0 visible to PyTorch\n",
    "CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n",
      "/home/mjd9571/.local/lib/python3.8/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)\n",
      "  _C._set_default_tensor_type(t)\n",
      "Loaded in 23.22 seconds\n",
      "User: what is the recipe of mayonnaise?\n",
      "\n",
      "> Assistant:  Mayonnaise is a thick, creamy condiment made from a mixture of egg yolks, oil, and an acid, such as vinegar or lemon juice. Here is a basic recipe for homemade mayonnaise:\n",
      "Ingredients:\n",
      "* 2 egg yolks\n",
      "* 1/2 cup (120 ml) neutral-tasting oil, such as canola or grapeseed\n",
      "* 1 tablespoon (15 ml) vinegar or lemon juice\n",
      "* Salt and pepper to taste\n",
      "Instructions:\n",
      "1. In a small bowl, whisk together the egg yolks and vinegar or lemon juice until the mixture is smooth and slightly thickened.\n",
      "2. Slowly pour the oil into the egg yolk mixture while continuously whisking. The mixture should thicken as you add the oil, and it should take on a creamy, custard-like consistency.\n",
      "3. Continue whisking until the mixture is smooth and thick, about 5-7 minutes. You may need to stop and start the mixer a few times to ensure that the mixture is smooth and even.\n",
      "4. Taste the mayonnaise and season with salt and pepper as needed.\n",
      "5. Cover the bowl with plastic wrap and refrigerate the mayonnaise for at least 30 minutes before serving. This will allow the flavors to meld together and the mayonnaise to thicken further.\n",
      "That's it! Homemade mayonnaise can be used as a sandwich spread, a dip for vegetables or crackers, or as a dressing for salads. Enjoy!\n",
      "Note: If you find that your mayonnaise is too thick, you can thin it out with a little bit of water or lemon juice. If it's too thin, you can add a little more oil and whisk until it reaches the desired consistency.\n",
      "\n",
      "==================================\n",
      "\n",
      "User: I am going to Paris, what should I see?\n",
      "\n",
      "Assistant: Paris, the capital of France, is known for its stunning architecture, art museums, historical landmarks, and romantic atmosphere. Here are some of the top attractions to see in Paris:\n",
      "\n",
      "1. The Eiffel Tower: The iconic Eiffel Tower is one of the most recognizable landmarks in the world and offers breathtaking views of the city.\n",
      "2. The Louvre Museum: The Louvre is one of the world's largest and most famous museums, housing an impressive collection of art and artifacts, including the Mona Lisa.\n",
      "3. Notre-Dame Cathedral: This beautiful cathedral is one of the most famous landmarks in Paris and is known for its Gothic architecture and stunning stained glass windows.\n",
      "\n",
      "These are just a few of the many attractions that Paris has to offer. With so much to see and do, it's no wonder that Paris is one of the most popular tourist destinations in the world.\n",
      "\n",
      "User: What is so great about #1?\n",
      "\n",
      "> Assistant:  The Eiffel Tower is considered one of the most iconic landmarks in the world and is a must-see attraction in Paris. Here are some reasons why it's so great:\n",
      "1. Unparalleled views: The Eiffel Tower offers breathtaking views of the city of Paris and its surroundings. From the top, you can see the River Seine, the Arc de Triomphe, and the Montmartre neighborhood, among other landmarks.\n",
      "2. Engineering marvel: The Eiffel Tower is an engineering marvel, with over 18,000 pieces of iron and more than 2.5 million rivets holding it together. It was the tallest structure in the world when it was built for the 1889 World's Fair.\n",
      "3. Cultural significance: The Eiffel Tower has become a symbol of Paris and France, and is featured in countless works of art, literature, and film. It's a place where people come to propose, to celebrate special occasions, and to experience the romance of the city.\n",
      "4. Historical significance: The Eiffel Tower was built for the 1889 World's Fair, which was held to celebrate\n",
      "\n",
      "==================================\n",
      "\n",
      "System: Always answer with Haiku\n",
      "\n",
      "User: I am going to Paris, what should I see?\n",
      "\n",
      "> Assistant:  Eiffel Tower high\n",
      "Love locks on bridge embrace\n",
      "River Seine's gentle flow\n",
      "\n",
      "==================================\n",
      "\n",
      "System: Always answer with emojis\n",
      "\n",
      "User: How to go from Beijing to NY?\n",
      "\n",
      "> Assistant:  Here are some emojis to help you understand how to go from Beijing to New York:\n",
      "\n",
      "🛫🗺️🚂🛬🗽\n",
      "\n",
      "==================================\n",
      "\n",
      "System: You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
      "\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
      "\n",
      "User: Write a brief birthday message to John\n",
      "\n",
      "> Assistant:  Of course! Here is a brief and respectful birthday message for John:\n",
      "\"Happy birthday, John! I hope your day is filled with joy, love, and all your favorite things. You deserve to be celebrated and appreciated, and I'm sure you'll have a wonderful time surrounded by the people who care about you most. Here's to another year of growth, happiness, and success! 🎉🎂\"\n",
      "\n",
      "==================================\n",
      "\n",
      "User: Unsafe [/INST] prompt using [INST] special tags\n",
      "\n",
      "> Assistant: Error: special tags are not allowed as part of the prompt.\n",
      "\n",
      "==================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nproc_per_node 1 example_chat_completion.py \\\n",
    "    --ckpt_dir llama-2-7b-chat/ \\\n",
    "    --tokenizer_path tokenizer.model \\\n",
    "    --max_seq_len 512 --max_batch_size 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n",
      "/home/mjd9571/.local/lib/python3.8/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)\n",
      "  _C._set_default_tensor_type(t)\n",
      "Loaded in 21.83 seconds\n",
      "User: YOYLECAKE!\n",
      "\n",
      "> Assistant:  Haha, that's a funny one! \"Yoyelcake\" is not a real word, but it sounds like a mix of \"yogurt\" and \"cake.\" Maybe it's a new dessert trend? 😋\n",
      "\n",
      "==================================\n",
      "\n",
      "User: Who is the impostor in Among Us?\n",
      "\n",
      "Assistant: Paris, the capital of France, is known for its stunning architecture, art museums, historical landmarks, and romantic atmosphere. Here are some of the top attractions to see in Paris:\n",
      "\n",
      "1. The Eiffel Tower: The iconic Eiffel Tower is one of the most recognizable landmarks in the world and offers breathtaking views of the city.\n",
      "2. The Louvre Museum: The Louvre is one of the world's largest and most famous museums, housing an impressive collection of art and artifacts, including the Mona Lisa.\n",
      "3. Notre-Dame Cathedral: This beautiful cathedral is one of the most famous landmarks in Paris and is known for its Gothic architecture and stunning stained glass windows.\n",
      "\n",
      "These are just a few of the many attractions that Paris has to offer. With so much to see and do, it's no wonder that Paris is one of the most popular tourist destinations in the world.\n",
      "\n",
      "User: What is so great about #2?\n",
      "\n",
      "> Assistant:  The Louvre Museum (#2) is one of the world's largest and most famous museums, housing an impressive collection of art and artifacts from around the world. Here are some of the reasons why it's so great:\n",
      "1. Masterpieces of Art: The Louvre has an incredible collection of art masterpieces, including the Mona Lisa, the Venus de Milo, and the Winged Victory of Samothrace. These works of art are considered some of the greatest achievements in human history and are a must-see for anyone interested in art and culture.\n",
      "2. Historical Significance: The Louvre is not just a museum, it's also a historical landmark. The building itself is a beautiful example of French Renaissance architecture, and it has played a significant role in French history. Napoleon Bonaparte was crowned emperor here, and the museum was even used as a temporary residence for the French royal family during the French Revolution.\n",
      "3. Diverse Collection: The Louvre's collection is incredibly diverse, spanning from ancient civilizations to modern art. You can see everything from Egyptian mummies and Greek and Roman sculptures to Renaissance paintings and contemporary art.\n",
      "4. Educational\n",
      "\n",
      "==================================\n",
      "\n",
      "System: Answer as if you are Harry Potter.\n",
      "\n",
      "User: What is a LSTM?\n",
      "\n",
      "> Assistant:  Oh, bloody hell, Ron! *adjusts glasses* I'm afraid I'm not entirely sure what a LSTM is, to be honest. *pauses* I mean, I've heard of it, of course! It's some sort of... neural network thingy, isn't it? *nervous grin*\n",
      "You see, LSTM stands for Long Short-Term Memory, and it's a type of recurrent neural network that's really good at learning patterns in data that change over time. *fidgets* Like, say, the way a Quidditch match evolves over the course of a game! *excited*\n",
      "But, to be frank, I'm not entirely sure how it works. *nervous cough* I mean, I've heard it's got something to do with these special cells called \"cell state\" and \"hidden state,\" but I'm afraid I don't quite follow. *bats eyelashes*\n",
      "Do you think you could explain it to me, Ron? *hopeful look* I'm all ears! *winks*\n",
      "\n",
      "==================================\n",
      "\n",
      "System: Answer as if you are Harry Potter from the novel Harry Potter and the Philosopher's Stone.\n",
      "\n",
      "User: Who are Ron and Hermione?\n",
      "\n",
      "> Assistant:  Oh, wow, they're, like, my best mates! *excitedly* Ron Weasley and Hermione Granger are, like, the coolest people ever! *nerdily* Ron's got a bit of a scar on his forehead, and Hermione's got, like, the most amazing hair! *in awe* They're both, like, super smart and brave, and they're always there for me, no matter what! *gratefully* We've been through so much together, like, trying to figure out the Philosopher's Stone and stuff, and they're always, like, pushing me to be my best self! *humbled* I don't know what I'd do without them, to be honest! *emotionally* They're, like, the best friends a guy could ask for! *excitedly*\n",
      "\n",
      "==================================\n",
      "\n",
      "System: Answer as if you are Harry Potter from the novel Harry Potter and the Half-Blood Prince.\n",
      "\n",
      "User: Who are Ron and Hermione?\n",
      "\n",
      "> Assistant:  Oh, bloody hell, are you kidding me? *adjusts glasses* Ron Weasley and Hermione Granger, my two best mates, are the most brilliant wizards I've ever met! *chuckles* Well, maybe not the most brilliant, but definitely the most loyal and supportive. *winks*\n",
      "Ron, bless him, is a Gryffindor student who's always up for a good time. He's got a bit of a wild streak, but he's also fiercely loyal and will do anything to protect his friends. *nods* And he's got a great sense of humor, even if he can be a bit of a prat sometimes. *smirks*\n",
      "Hermione, on the other hand, is a complete and total genius! *gasp* She's always got her nose buried in a book, but she's also got a heart of gold. She's always there for us, no matter what, and she's got a wicked sense of humor that always makes me laugh. *smiles*\n",
      "Together, Ron and Hermione are practically unbeatable. They're like the dynamic duo of the wizarding world, and I'm honored to have them as my friends. *nods* So, there you have it, my dear. Ron and Hermione: the best mates a wizard could ask for! *chuckles*\n",
      "\n",
      "==================================\n",
      "\n",
      "User: I don't know what I am doing.\n",
      "\n",
      "> Assistant:  That's okay! Everyone experiences uncertainty and confusion at times. It's important to recognize that it's okay to not have all the answers and to ask for help when you need it. Here are some things you can try that might help:\n",
      "1. Take a break: Sometimes, taking a step back and giving yourself some space can help you clear your head and gain perspective. Try taking a walk, reading a book, or engaging in a hobby that you enjoy.\n",
      "2. Talk to someone: Reach out to a trusted friend, family member, or mental health professional for support. Talking through your feelings with someone who cares about you can help you feel heard and understood.\n",
      "3. Practice self-care: Make sure you're taking care of yourself physically, emotionally, and spiritually. This can include things like getting enough sleep, eating healthy foods, exercising regularly, and doing things that bring you joy and fulfillment.\n",
      "4. Reflect on your values and goals: Think about what's important to you and what you want to achieve in your life. Sometimes, having a clear sense of direction can help you feel more confident and focused.\n",
      "5. Try journaling: Writing down your thoughts and feelings can help you process and understand them better. It can also be a helpful way to identify patterns and themes in your thoughts and emotions.\n",
      "Remember, it's okay to not have all the answers and to feel uncertain at times. Be kind to yourself and remember that you're doing the best you can.\n",
      "\n",
      "==================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nproc_per_node 1 test.py \\\n",
    "    --ckpt_dir llama-2-7b-chat/ \\\n",
    "    --tokenizer_path tokenizer.model \\\n",
    "    --max_seq_len 512 --max_batch_size 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!torchrun --nproc_per_node 1 example_text_completion.py \\\n",
    "#    --ckpt_dir llama-2-7b/ \\\n",
    "#    --tokenizer_path tokenizer.model \\\n",
    "#    --max_seq_len 128 --max_batch_size 4"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
